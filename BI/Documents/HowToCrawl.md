## 크롤링 원리의 이해

### 1. 각 사이트별로 아이템들을 중요도 순으로 줄을 세운다.
* 중요도는 기획전 등에 포함된 것인지, 메인에 노출되었는지 등의 기준을 가지고 있다. (정확한 것은 파악 안 됨)
* 마지막으로 크롤된 뒤 시간이 지나면서 중요도는 커진다

### 2. 중요도가 높은 순으로 하루에 일정량을 크롤한다.
* 사이트별로 크롤가능한 아이템의 수가 다르다. <br>
ex) GSshop의 경우 하루 30만개를 크롤할 수 있고, 총 아이템의 수는 2~3백만 개 가량이다

### 3. 한 ITEM_ID 내의 STOCK들은 한 번에 크롤한다.
* STOCK은 일반적으로 한 ITEM의 옵션(색, 사이즈 등)을 가리킨다.
* 한 ITEM을 긁을 때, 일부의 STOCK만을 긁지 않고 존재하는 모든 STOCK을 다 긁는다.
* 각 STOCK에 대한 REG_DT는 대개 동일하지만, 통신 등의 이유로 1~2초 정도 차이날 수도 있다.
  * 이를 보정하는 함수가 modify_1sec
  
### 4. BS_URL 테이블의 SELL_END_YN 칼럼
* 이를 통해 각 ITEM이 현재 크롤되고 있는지의 여부를 판단할 수 있다.
* 다만 그 신뢰도는 100% 믿을 수 없다. 현재 정확성을 진단하려 데이터를 검수 중
* 하나의 BS_URL에 수백 개의 ITEM_NUM이 존재할 수 있다.
  * 하지만 매핑되는 ITEM_NUM은 단 하나 뿐이다 ...
  * 하나의 Base URL을 공유하는 ITEM_NUM들의 COLLECT_URL(MWS_COLT_ITEM 테이블의 칼럼)은 동일하다.
    * 그러나 인덱싱이 되어있지 않아 찾는 것이 심각하게 느리다 (10분 이상)
    * hive에서 URL로 다시 파티셔닝을 할까?
    
* * *

## Smart Crawler의 이해

### 1. Smart Crawler가 진단하려는 것
* 현재보다 더 자주 크롤되어야 하는 것
* 이 부분에 대한 정의가 가장 어렵다.

### 2. Smart Crawler가 실제 하는 것
* 찾아낸 아이템들을 EXECUTE_PLAN 테이블에 올린다.
* 어떤 아이템을(ITEM_NUM) 언제(ACT_DT) 어디서(COLLECT_SITE) 크롤할 것인지를 등록(REG_DT)
  * queuing_execute_plan을 참조
  * pandas가 지원하는 rolling이나 data_range 함수가 유용하다.
* EXECUTE_PLAN에 큐를 넣을 때는 개발팀에 미리 말을 **꼭!!** 해야 한다.
  * Default로는 꺼져있다.

### 3. 부차적인 문제
* 크롤이 종료된 아이템을 EXECUTE_PLAN에 넣으면 좋지 않다
  * 잘못되면 크롤링 전체가 fail될 수 있다. 이를 검증하기 위해 BS_URL의 SELL_END_YN 칼럼을 참조하려 하지만 완전히 신뢰하기 어렵다.
  
